{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7bMcCx0sC7T",
        "outputId": "f3153f35-ca18-4792-94d8-e437262faa24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.24.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "\n",
        "# Kindly add all your installations and versions if any in this cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import csv\n",
        "from scipy import stats\n",
        "from sklearn import linear_model\n",
        "from gensim.models import FastText\n",
        "from sentence_transformers import SentenceTransformer, losses, models, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "from gensim.test.utils import common_texts \n",
        "import torch \n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "tR58sjjtsKdb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FePza_fqgY-9",
        "outputId": "6e637739-1b4f-4721-a7a2-60b082d291f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.8/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.8/dist-packages (from fasttext) (2.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext"
      ],
      "metadata": {
        "id": "hc-7GtrAgVcu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import utils\n",
        "from gensim.models.doc2vec import LabeledSentence\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy67kEKmsKgb",
        "outputId": "8a58bbb3-0309-48e8-9c29-5d8268d086c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_PATH=\"drive/MyDrive/NLP Ass3 stsbenchmark/\"\n",
        "def read_sts_csv(dataset_type=\"train\", columns=['source', 'type', 'year', 'id', 'score', 'sent_a', 'sent_b']):\n",
        "  path = INPUT_PATH + \"sts-\"+ dataset_type + \".csv\"\n",
        "  return pd.read_csv(path,sep=\"\\t\",header=None, names=columns,quoting=csv.QUOTE_NONE)\n",
        "\n",
        "data_train = read_sts_csv(\"train\") # create the train, dev and test dataframes\n",
        "data_dev = read_sts_csv(\"dev\") # create the train, dev and test dataframes\n",
        "data_test = read_sts_csv(\"test\") # create the train, dev and test dataframes\n",
        "\n",
        "# final_data=data_train.copy(deep=True)\n",
        "# print(len(data_train))\n",
        "# final_data=final_data.append(data_dev,ignore_index=True)\n",
        "# print(len(data_dev))\n",
        "# final_data=final_data.append(data_test,ignore_index=True)\n",
        "# print(len(data_test))\n"
      ],
      "metadata": {
        "id": "mXq58QYYtHzD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def review_to_wordlist(review, remove_stopwords=True):\n",
        "    # Clean the text, with the option to remove stopwords.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    words = review.lower().split()\n",
        "\n",
        "    # Optionally remove stop words (true by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    \n",
        "    review_text = \" \".join(words)\n",
        "\n",
        "    # Clean the text\n",
        "    review_text = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\'s\", \" 's \", review_text)\n",
        "    review_text = re.sub(r\"\\'ve\", \" 've \", review_text)\n",
        "    review_text = re.sub(r\"n\\'t\", \" 't \", review_text)\n",
        "    review_text = re.sub(r\"\\'re\", \" 're \", review_text)\n",
        "    review_text = re.sub(r\"\\'d\", \" 'd \", review_text)\n",
        "    review_text = re.sub(r\"\\'ll\", \" 'll \", review_text)\n",
        "    review_text = re.sub(r\",\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\.\", \" \", review_text)\n",
        "    review_text = re.sub(r\"!\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\(\", \" ( \", review_text)\n",
        "    review_text = re.sub(r\"\\)\", \" ) \", review_text)\n",
        "    review_text = re.sub(r\"\\?\", \" \", review_text)\n",
        "    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
        "    \n",
        "    words = review_text.split()\n",
        "    \n",
        "    # Shorten words to their stems\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    \n",
        "    review_text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    # print(review_text)\n",
        "    return(review_text)"
      ],
      "metadata": {
        "id": "SXJsV8rTtH4O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[]\n",
        "list2=[]\n",
        "for i in data_train['sent_a']:\n",
        "  list1.append(review_to_wordlist(i))\n",
        "data_train['sent_a']=list1\n",
        "for i in data_train['sent_b']:\n",
        "  list2.append(review_to_wordlist(i))\n",
        "data_train['sent_b']=list2"
      ],
      "metadata": {
        "id": "ISs0QuK_tH6z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[]\n",
        "list2=[]\n",
        "for i in data_test['sent_a']:\n",
        "  list1.append(review_to_wordlist(i))\n",
        "data_test['sent_a']=list1\n",
        "for i in data_test['sent_b']:\n",
        "  list2.append(review_to_wordlist(i))\n",
        "data_test['sent_b']=list2"
      ],
      "metadata": {
        "id": "Ntxl7-8qq0XW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_data['sent_a'][1]"
      ],
      "metadata": {
        "id": "0xbRCaVAtH9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Contains the processed questions for Doc2Vec\n",
        "# sent_labeled = []\n",
        "\n",
        "# for i in range(len(final_data['sent_a'])):\n",
        "#     # Question strings need to be separated into words\n",
        "#     # Each question needs a unique label\n",
        "#     sent_labeled.append(final_data['sent_a'][i].split())\n",
        "#     sent_labeled.append(final_data['sent_b'][i].split())"
      ],
      "metadata": {
        "id": "7IEeFl4OtIAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(sent_labeled[5000])"
      ],
      "metadata": {
        "id": "Gvfr_DJKtICm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Word2Vec(sent_labeled, size=100, window=5,min_count=1,sg = 1,hs = 0,negative = 10,seed = 34)\n",
        "# model.train(sent_labeled, total_examples= len(sent_labeled), epochs=20)"
      ],
      "metadata": {
        "id": "33JOT6PNt7sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Defining values for parameters\n",
        "# embedding_size = 200\n",
        "# window_size = 5\n",
        "# min_word = 1\n",
        "# down_sampling = 1e-2\n",
        " \n",
        "# fast_Text_model = FastText(sent_labeled,\n",
        "#                       size=embedding_size,\n",
        "#                       window=window_size,\n",
        "#                       min_count=min_word,\n",
        "#                       sample=down_sampling,\n",
        "#                       sg=1,\n",
        "#                       iter=100)"
      ],
      "metadata": {
        "id": "0p1Httf9Qicq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = FastText(size=100, window=3, min_count=1)  # instantiate\n",
        "# model.build_vocab(corpus_iterable=common_texts)"
      ],
      "metadata": {
        "id": "WTek4FpETDKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import fasttext\n",
        "# import io\n",
        "\n",
        "# def load_vectors(fname):\n",
        "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "#     n, d = map(int, fin.readline().split())\n",
        "#     data = {}\n",
        "#     for line in fin:\n",
        "#         tokens = line.rstrip().split(' ')\n",
        "#         data[tokens[0]] = map(float, tokens[1:])\n",
        "#     return data\n",
        "\n",
        "# vectors = load_vectors('drive/MyDrive/NLP Ass3 stsbenchmark/wiki-news-300d-1M.vec')\n",
        "# model = fasttext.load_model(vectors)"
      ],
      "metadata": {
        "id": "doQ2pLQfhjQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this from a normal command line\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmYjqoYVna8P",
        "outputId": "4ba7916a-920e-4629-80ca-db58685e85c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-01 21:51:02.709494: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spacy model that you have installed\n",
        "model = spacy.load('en_core_web_md')\n",
        "\n",
        "# process a sentence using the model\n",
        "doc = model('larg')\n",
        "\n",
        "# It's that simple - all of the vectors and words are assigned after this point\n",
        "# Get the vector for 'text':\n",
        "doc.vector\n",
        "\n",
        "# Get the mean vector for the entire sentence (useful for sentence classification etc.)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvwclC62RUNq",
        "outputId": "82cef010-f306-462c-a32f-fd920013f02c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.1461  ,   0.14249 ,  -1.0703  ,   4.9461  ,   4.548   ,\n",
              "        -0.045589,   0.68434 ,  11.17    ,   2.212   ,   0.3946  ,\n",
              "         7.1267  ,   4.9683  ,  -3.8074  ,   3.446   ,   2.4072  ,\n",
              "         3.9904  ,  -1.2473  ,   2.4176  ,   2.8811  ,  -3.4609  ,\n",
              "        -1.5014  ,   3.0831  ,  -3.3356  ,   1.4513  ,   0.69302 ,\n",
              "        -2.015   ,  -2.8329  ,  -4.8632  ,  -0.91891 ,  -0.10473 ,\n",
              "        -1.2128  ,  -1.0851  ,  -1.7969  ,  -5.7501  ,  -3.8089  ,\n",
              "        -6.8127  ,   2.0525  ,   0.27913 ,   2.2762  ,   0.2932  ,\n",
              "         2.9304  ,   1.1093  ,   1.009   ,   0.36559 ,  -2.7335  ,\n",
              "         1.9654  ,  -0.46479 ,  -0.98216 ,  -2.2408  ,   4.7887  ,\n",
              "        -0.38744 ,   2.0715  ,   0.42249 ,  -2.2575  ,  -3.6327  ,\n",
              "        -1.0375  ,   0.54707 ,   3.4792  ,   6.4351  ,  -2.7992  ,\n",
              "         0.14296 ,  -0.054221,   5.4176  ,  -0.12402 ,   4.2284  ,\n",
              "        -0.71569 ,   1.4477  ,  -0.42726 ,   1.6489  ,  -0.41928 ,\n",
              "         0.48156 ,  -3.6034  ,  -6.8822  ,   3.8116  ,  -2.3845  ,\n",
              "        -3.7417  ,  -4.4009  ,  -0.11012 ,  -0.88915 ,  -0.06513 ,\n",
              "       -10.799   ,  -0.11361 ,   1.9908  ,   6.7592  ,   6.0018  ,\n",
              "         2.4266  ,  -0.12843 ,   1.3727  ,   1.3206  ,  -2.5023  ,\n",
              "        -4.1528  ,  -1.8695  ,   0.2303  ,  -7.0213  ,   2.4184  ,\n",
              "        -1.819   ,   8.1661  ,  -2.0895  ,   0.4905  ,   7.2003  ,\n",
              "         1.9547  ,   0.638   ,   1.4069  ,   4.3386  ,  -2.9226  ,\n",
              "         2.9473  ,   0.72702 ,  -1.6225  ,   1.8092  ,  -4.9998  ,\n",
              "         0.60631 ,   6.8462  ,  -0.64185 ,  -2.9888  ,  -1.0598  ,\n",
              "         1.0919  ,   3.1235  ,  -0.40048 ,   6.6825  ,  -2.4137  ,\n",
              "        -2.5215  ,  -0.24633 ,  -1.0099  ,   5.3933  ,  -5.9422  ,\n",
              "        -9.8285  ,  -2.7474  ,  -2.2014  ,   5.378   ,   3.0387  ,\n",
              "        -7.8199  ,   6.5524  ,   7.1353  ,   3.0559  ,   4.4403  ,\n",
              "         3.1092  ,   0.95133 ,  -5.0921  ,  -2.838   ,  -2.0961  ,\n",
              "        -1.3402  ,   5.3434  ,  -2.5991  ,  -0.56511 ,   4.4869  ,\n",
              "        -2.7398  ,  -3.0428  ,   1.6736  ,   1.9784  ,  -0.10305 ,\n",
              "        -2.557   ,   3.3502  ,  -0.67478 ,   3.7945  ,   6.4082  ,\n",
              "        -0.97495 ,  -0.029559,   0.26599 ,  -0.74066 ,  -3.1162  ,\n",
              "        -1.585   ,  -1.1274  ,   5.6959  ,   2.4     ,  -0.86368 ,\n",
              "         4.7914  ,  -1.3672  ,   4.5797  ,   4.7382  ,  -1.0742  ,\n",
              "        -0.28352 ,   0.95876 ,  -1.2451  ,   1.96    ,  -2.5655  ,\n",
              "        -2.5405  ,  -1.5582  ,  -1.4459  ,  -3.8588  ,   0.16325 ,\n",
              "        -2.1994  ,   1.4657  ,  -8.1987  ,  -1.4404  ,  -5.665   ,\n",
              "        -0.86259 ,  -0.83035 ,   3.2634  ,  -2.554   ,   0.77386 ,\n",
              "        -0.90199 ,  -3.1249  ,  -0.30673 ,   0.15135 ,   3.9175  ,\n",
              "         6.4957  ,  -4.1809  ,  -6.5898  ,   0.51387 ,  -1.8676  ,\n",
              "        -1.8662  ,  -3.5622  ,   1.6286  ,   1.5747  ,   5.3328  ,\n",
              "        -0.31774 ,  -8.4468  ,   5.1595  ,   3.7676  ,  -3.1855  ,\n",
              "         1.9957  ,   4.2504  ,  -2.16    ,  -3.8765  ,   2.1981  ,\n",
              "         3.987   ,   6.5014  ,  -6.8875  ,  -7.2328  ,   2.4718  ,\n",
              "         0.035396,   6.2622  ,  -2.2743  ,  -7.3216  ,  -3.762   ,\n",
              "         0.53198 ,  -1.9646  ,   3.0247  ,   6.6449  ,  -2.8473  ,\n",
              "         8.0611  ,   1.4558  ,   3.5339  ,   3.8343  ,  -2.2638  ,\n",
              "         1.9917  ,   2.2749  ,  -5.0905  ,   1.4443  ,   1.2099  ,\n",
              "         2.0039  ,   0.50511 ,  -4.1555  ,   3.5698  ,  -3.65    ,\n",
              "        -1.199   ,   1.8166  ,  -0.12512 ,   1.3389  ,   1.3226  ,\n",
              "         2.3685  ,  -3.4276  ,  -8.5131  ,   0.49145 ,  -1.0261  ,\n",
              "       -10.37    ,   1.4742  ,   0.89152 ,  -3.9073  ,  -2.1836  ,\n",
              "         2.7559  ,  10.45    ,   5.7831  ,   0.055017,   3.245   ,\n",
              "        -2.6049  ,   1.7706  ,   6.7408  ,  -5.2665  ,   0.64377 ,\n",
              "         3.4987  ,  -0.59857 ,   0.52904 ,   1.6175  ,  -0.37467 ,\n",
              "        -4.1986  ,   1.7392  ,   0.9242  ,  -6.2227  ,   3.7284  ,\n",
              "         1.7476  ,  -2.8985  ,   6.5795  ,  -5.7616  ,   1.7015  ,\n",
              "        -3.4608  ,  -1.0122  ,   1.0393  ,  -3.4137  ,  -1.0201  ,\n",
              "        -5.2366  ,  -1.1696  ,  -1.2279  ,   4.6998  ,  -4.7141  ,\n",
              "         2.88    ,   0.79019 ,  -3.5631  ,  -5.9937  ,   1.2214  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# sent_a_split = []\n",
        "# for i in range(len(data_train['sent_a'])):\n",
        "#     sent_a_split.append(data_train['sent_a'][i].split())\n",
        "    \n",
        "# sent_b_split = []\n",
        "# for i in range(len(data_train['sent_b'])):\n",
        "#     sent_b_split.append(data_train['sent_b'][i].split())"
      ],
      "metadata": {
        "id": "3omJTT9fxkkt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# sent_a_split_test = []\n",
        "# for i in range(len(data_test['sent_a'])):\n",
        "#     sent_a_split_test.append(data_test['sent_a'][i].split())\n",
        "    \n",
        "# sent_b_split_test = []\n",
        "# for i in range(len(data_test['sent_b'])):\n",
        "#     sent_b_split_test.append(data_test['sent_b'][i].split())"
      ],
      "metadata": {
        "id": "zbUIG5lixkns"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent_a_split)"
      ],
      "metadata": {
        "id": "vbEHBRMau-he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sent_vect_all=[]\n",
        "# sent_vect=[]\n",
        "# for j in range(len(sent_a_split)):\n",
        "#   print(j)\n",
        "#   sent_vect=[]\n",
        "#   sum=0\n",
        "#   for i in sent_a_split[j]:\n",
        "#     sum+=model(i).vector\n",
        "#   first_vect=sum/len(sent_a_split[j])\n",
        "\n",
        "#   sum=0\n",
        "#   for i in sent_b_split[j]:\n",
        "#     sum+=model(i).vector\n",
        "#   second_vect=sum/len(sent_b_split[j])\n",
        "#   final_vect=(first_vect+second_vect)/2\n",
        "#   # final_vect=np.append(first_vect,second_vect)\n",
        "#   sent_vect_all.append(final_vect)"
      ],
      "metadata": {
        "id": "nPT23qWkxksM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sent_vect_all_test=[]\n",
        "# sent_vect=[]\n",
        "# for j in range(len(sent_a_split_test)):\n",
        "#   sent_vect=[]\n",
        "#   sum=0\n",
        "#   for i in sent_a_split_test[j]:\n",
        "#     sum+=model(i).vector\n",
        "#   first_vect=sum/len(sent_a_split_test[j])\n",
        "\n",
        "#   sum=0\n",
        "#   for i in sent_b_split_test[j]:\n",
        "#     sum+=model(i).vector\n",
        "#   second_vect=sum/len(sent_b_split_test[j])\n",
        "\n",
        "#   final_vect=(first_vect+second_vect)/2\n",
        "#   # final_vect=np.append(first_vect,second_vect)\n",
        "#   sent_vect_all_test.append(final_vect)"
      ],
      "metadata": {
        "id": "IO7XD4n0xlC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data_train['sent_a'])):\n",
        "  sum=0\n",
        "  j1=model(data_train['sent_a'][i]).vector\n",
        "  j2=model(data_train['sent_b'][i]).vector\n",
        "  final_vect=np.append(j1,j2)\n",
        "  sent_vect_all.append(final_vect)\n"
      ],
      "metadata": {
        "id": "QSlOL4risN1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data_test['sent_a'])):\n",
        "  sum=0\n",
        "  j1=model(data_test['sent_a'][i]).vector\n",
        "  j2=model(data_test['sent_b'][i]).vector\n",
        "  final_vect=np.append(j1,j2)\n",
        "  sent_vect_all.append(final_vect)"
      ],
      "metadata": {
        "id": "k27eREUOsN6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent_vect_all_test))"
      ],
      "metadata": {
        "id": "vSothQ3mxlFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train, x_train = data_train['score'],sent_vect_all"
      ],
      "metadata": {
        "id": "SUA4HrNmxqWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test, x_test = data_test['score'],sent_vect_all_test"
      ],
      "metadata": {
        "id": "FxfPsXRexqYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "y=regr.fit(x_train, y_train)\n",
        "from scipy.stats import spearmanr\n",
        "print()"
      ],
      "metadata": {
        "id": "TJSWHGQOxu6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred=y.predict(x_train)"
      ],
      "metadata": {
        "id": "p5n6xTsexu85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred_test=y.predict(x_test)"
      ],
      "metadata": {
        "id": "fuH68RLrx4sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ypred)\n",
        "coef, p = spearmanr(ypred, y_train)"
      ],
      "metadata": {
        "id": "G3glApKmxu_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ypred_test)\n",
        "coeft, pt = spearmanr(ypred_test, y_test)"
      ],
      "metadata": {
        "id": "R17xPR6Fxys9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(coef)"
      ],
      "metadata": {
        "id": "6m6b1cJvxyvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(coeft)"
      ],
      "metadata": {
        "id": "UERy77BaxyzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_texts)"
      ],
      "metadata": {
        "id": "vagMuyCSxy2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWzWGIa1xy5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}